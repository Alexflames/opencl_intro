\documentclass[bachelor, och, times]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage{xcolor}
\usepackage[english,russian]{babel}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Вычисления на видеокартах}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Григорьева Алексея Александровича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент} %должность, степень, звание
\saname{М.\,С.\,Семенов}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
\patitle{к.\,ф.-м.\,н., доцент}
\paname{Д.\,Ю.\,Петров}

% Семестр (только для практики, для остальных
% типов работ не используется)
\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{01.07.2016}
\practFinish{14.07.2016}

% Год выполнения отчета
\date{2019}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations
%\begin{description}
    %\item \foreignlanguage{english}{EVM} "--- %\foreignlanguage{english}{Ethereum Virtual Machine};
%\end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro
Ранние видеокарты использовались для решения узкоспециализированных задач по отображению
графических элементов на экране. До определенного момента их развитие происходило раздельно с процессорами, 
и функционал определялся стремительно развивавшейся игровой индустрией. С появлением у разработчиков
компьютерных игр желания самостоятельно программировать шейдеры, были разработаны программные
средства, способные решать широкий спектр задач. 

Одновременно с этим рост производительности процессоров сильно замедлился, и в начале 2000"=х закон Мура перестал действовать.
Производительность было решено увеличивать за счет
увеличения количества ядер, либо за счет некоторых оптимизаций в них. Однако, для задач с крупными вычислениями
этого было недостаточно, и сравнения производительности видеокарт и процессоров показали большую разницу в показателях~\ref{flops}.

Причиной этого является различие в количестве ядер, ставшее следствием слабой масштабируемости процессоров из"=за проблем
синхронизации потоков~\cite{performance}. 

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.35]{screenshots/flops}
	\caption{Теоретический максимум количества операций с плавающей запятой в такт для GPU и CPU на 2016 год. 
	График наилучшего по производительности процессора изображен снизу.}\label{flops}
\end{figure}  

Можно заметить, что небольшое количество многофункциональных ядер процессоров не превосходит по производительности видеокарты,
устроенные таким образом чтобы сотни небольших ядер работали параллельно.

В связи с тем что видеокарты созданы под решение определенного класса задач, алгоритмы должны обладать
определенными свойствами чтобы решение с использованием GPU было эффективней чем на процессоре. В данной работе
рассматривается архитектура вычислительных устройств видеокарты и связанных с ними особенностями, которые
влияют на проектирование алгоритмов.

В практической части рассматриваются не только реализация алгоритмов на видеокарте, но и возможные 
оптимизации по количеству вычислений и обращений к памяти.

При выполнении курсовой работы были поставлены следующие цели:
\begin{itemize}
	\item ознакомиться с теорией, необходимой для написания эффективных алгоритмов для видеокарты 
	с использованием OpenCL;
	\item понять свойства архитектуры видеокарты и тем самым научиться оптимизировать алгоритмы;
	\item получить практический опыт разработки программ на видеокартах с помощью OpenCL;
	\item провести исследование производительности параллельных программ на различных видеокартах.
\end{itemize}

\section{Краткая теория}
Составление эффективных алгоритмов вычисления на видеокарте в значительной степени отличается 
от привычных алгоритмов, исполняющихся на процессоре. При составлении программного кода необходимо 
учитывать как и общие особенности видеокарт, так и, возможно, характеристики конкретного устройства, 
для которого программируется алгоритм.

В данном разделе будет рассмотрена типовая модель видеокарты и основные понятия \foreignlanguage{english}{OpenCL},
с которыми будем оперировать в данной работе.

\subsection{Типовая модель видеокарты}
Рассмотрим следующую архитектуру вычислительного устройства, используемого в видеокартах
\foreignlanguage{english}{Nvidia} \ref{SM}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{screenshots/videocard-base}
	\caption{Архитектура потокого мультипроцессора Fermi.}\label{SM}
\end{figure}

Вычислительное устройство в архитектуре Nvidia имеет 32 \textbf{ядра} (CUDA cores), 
каждое из которых в состоянии работы является \textbf{потоком}. 
В отличии от процессора, ядра в видеокарте выполняют более узкий набор задач, что позволяет с меньшими затратами 
увеличить их количество в устройстве\cite{fermi}. 
Для управления ими существует \textbf{warp scheduler}, выполняющий роль указателя на инструкции
соответствуя архитектуре SIMD. Данные для вычислений потоки берут из \textbf{локальной памяти} (shared memory), общей для
всех ядер. Достигается это с использованием \textbf{устройств загрузки и хранения} (load-store units), соответственно
способных загружать, а также сохранять данные в локальную память. Между всеми 32 ядрами вычислительного
устройства динамически распределяются
\textbf{регистры}, самая быстрая память, доступная им. У мультипроцессора в наличии намного больше регистров, чем могло быть
нужно для выполнения программы. Это сделано для сокрытия задержки на загрузку памяти и быстрого переключения контекста,
подробнее - в разделе~\ref{effectiveness}.
 
Количество вычислительных устройств в видеокарте определяется следующим соотношением:
\begin{equation*}
\text{Количество ядер в видеокарте / } 32  \text{, в случае Nvidia}
\end{equation*}
\begin{equation*}
\text{Количество ядер в видеокарте / } 64  \text{, в случае AMD}
\end{equation*}

В терминологии Nvidia, поток из всех (32) активных ядер вычислительного устройства образует \textbf{warp}, 
Например, видеокарта Nvidia Geforce GTX 1050 Ti имеет 768 ядер CUDA, и, соответственно, 24 вычислительных
устройств. В видеокартах AMD в одном вычислительном устройстве 64 ядра, образующие во время выполнения \textbf{wavefront}

\subsection{Основные понятия OpenCL}\label{general-opencl}

\textbf{OpenCL} "--- открытый для свободного пользования программный интерфейс для создания параллельных приложений, 
использующих многоядерные структуры как и центрального процессора (CPU), так и графического (GPU).
Использование API необходимо для обеспечения совместимости программы с различными устройствами\cite{opencl-intro}.

При построении задач, определяется \textbf{рабочее пространство} (NDRange), представляющее собой все возможные в рамках 
задачи значения индексов потоков. 
Размер рабочего пространства определяется программистом на этапе инициализации OpenCL программы.
Рабочее пространство может представлять:
\begin{itemize}
	\item одномерный массив длиной N элементов;
	\item двумерную сетку размерности NxM;
	\item трехмерное пространство размерностью NxMxP.
\end{itemize}

Код, выполняющийся параллельно на ядрах процессора, называется \textbf{kernel}. Копия kernel выполняется
для каждого потока в рабочем пространстве и называется \textbf{work-item} с глобальным ID, соответствующим
некоторому ID рабочего пространства. Kernel для всех work-item в рабочем пространстве имеют одинаковый код
и входные параметры, но может иметь различный путь выполнения программы соответственно своему глобальному индексу -
индекс в рабочем пространстве, полученному с использованием функции \verb|get_global_id()|. Kernel в отличии
от остальной программы полностью выполняется на видеокарте\cite{opencl-spec}.

Группа work-item называется \textbf{work-group}, и за каждой группой закреплен собственный warp (см. предыдущий раздел),
в рамках которого work-item могут синхронизироваться. Для каждой рабочей группы существует ее индекс в рабочем
пространстве, и каждый work-item может узнать свой локальный индекс внутри рабочей группы. Нетрудно заметить следующее 
соотношение:
\begin{equation*}
	\text{global ID } = \text{ group ID * размер группы } + \text{ local ID} 
\end{equation*}
Размер рабочей группы аналогично рабочему пространству определяется программистом.

Используя приведенную выше терминологию, можно сказать что каждое ядро, выполняя заданный kernel, 
является work-item в некоторой рабочей группе,
на которые разделено рабочее пространство NDRange.

Рассмотрим на примере следующей схемы~\ref{entities} другие виды сущностей, c которыми будет взаимодействие 
в OpenCL.
\begin{itemize}
	\item Платформа "--- драйвер, модель взаимодействия OpenCL и устройства. Распространены платформы от следующих
	производителей: Nvidia, Intel, AMD.
	\item Программа "--- хостовая часть, организующая подготовку к вычислениям и набор kernel-подпрограмм.
	\item Kernel "--- программа, исполняющаяся на видеокарте в каждом ядре.
	\item Контекст "--- окружение, в котором исполняется kernel.
	\item Объект памяти "--- создаваемый в контексте объект.
	\item Буфер "--- произвольный массив данных.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{screenshots/opencl-entities}
	\caption{Основные сущности в OpenCL.}\label{entities}
\end{figure}

\section{Алгоритмы на видеокарте}
В данном разделе будет рассмотрена анализ и практическая реализация алгоритмов на видеокарте, включая:
\begin{itemize}
	\item описание общих требований к алгоритмам на основе доступа к памяти и параллельного исполнения;
	\item настройка среды разработки Microsoft Visual Studio 2017 под выполнение параллельных программ с использованием OpenCL;
	\item написание программ для задач, использующих входные данные разных размерностей.
	\item сравнение времени выполнения параллельных программ на различных видеокартах,
	а также с последовательными вычислениями на процессоре.
\end{itemize}


\subsection{Требования к алгоритмам} \label{effectiveness}
Любой алгоритм можно вычислить на видеокарте, но эффективность в сравнении с реализацией на 
центральном процессоре зависит от корректности построения алгоритма для видеокарты.

Основным требованием к составлению алгоритма на видеокарте считается наличие 
\textbf{массового параллелизма}. Он заключается в том что задачу можно разбить на рабочие группы так, что
не будет требоваться постоянная синхронизация между work-item из разных рабочих групп.

Следует вспомнить, что все потоки в warp выполняют одинаковые инструкции в любой момент
времени. Какая инструкция будет выполняться следующей определяется с помощью warp scheduler, единого
для всех потоков в warp. Рассмотрим следующий фрагмент кода:

\begin{Verbatim}
if (predicate) {
    value = x[i];
}
else {
    value = y[i];
}
\end{Verbatim}

Учитывая сказанное выше, все потоки при срабатывания if-части должны выполнить внутреннюю часть,
однако это не совсем так, и если у потока предикат "--- \verb|False|, он будет спрятан от выполнения
внутренней части, аналогично и для \verb|else|-части. Однако, несмотря на то что результат выполнения 
конструкции \verb|if-else| будет верным, часть потоков будет простаивать, ожидая выполнение
маскированных для них частей. 

Данная ситуация называется code divergence, и она может стать причиной низкой производительности программы.
Этого можно избежать, если организовать код таким образом чтобы для всех потоков предикат
возвращал одинаковое значение, тогда конструкция не соответствующая ему будет пропущена указателем на инструкции.
Если это невозможно, то для эффективного выполнения алгоритма рекомендуется отказаться от многочисленных
ветвлений, так как сложность выполнения фрагмента алгоритма будет вычисляться как сумма if- и else- частей
вместо максимума как в последовательных программах.  

При выборе размера рабочих групп стоит учитывать особенности алгоритма, однако, есть
некоторые общие правила, которых необходимо придерживаться.
\begin{enumerate}
	\item Размер рабочей группы не должен быть меньше warp.
	\item Размер рабочей группы должен быть кратен 32 (64 если используется AMD).
\end{enumerate}

В противном случае, некоторые потоки будут простаивать, ожидая пока остальные в данной 
рабочей группе завершат свою работу.

Как известно, операции с памятью являются одними из самых долгих по времени выполнения.
В связи с этим было решено сделать разбиение задач на рабочие группы, и в результате
у видеокарт появился аналог имеющегося у процессоров hyper-threading. Он заключается в 
использовании каждым вычислительным устройством \textbf{регистров} для переключения контекста при задержке, созданной
обращением к памяти (latency)\cite{videocard-architecture}.

Другими словами, warp может быстро сохранить в регистровую память состояние
выполнения в данной рабочей группе, и пока выполняется долгая операция обращения к памяти, вычислительное
устройство может переключиться на другой warp в рабочей группе. Если второй warp хочет выполнить
операцию обращения к памяти, то происходит возвращение к первому warp при условии что доступ к памяти у него завершился, либо
активируется третий warp и так далее. Следствие "--- высокая вычислительная мощность 
и большая пропускная способность видеокарты\cite{warps}. 

Количество одновременно активных warp в рабочей группе определяется как минимум из:
\begin{itemize}
	\item количества регистров / количество используемых в warp регистров;
	\item количества локальной памяти / количество используемой локальной памяти;
	\item максимально допустимого количества warp (\verb|~10|).
\end{itemize}

В соответствии с этим существует величина \textbf{occupancy}, определяемая соотношением
\begin{equation*}
	\text{среднее кол-во активных warp } / \text{ максимальное кол-во активных warp} 
\end{equation*}

Не всегда высокий occupancy означает что программа имеет высокую производительность.
Например, если доступ к памяти в программе очень быстрый, только из регистров, 
то необходимости в сокрытии задержки и переключения контекста нет, и occupancy будет низким.

Однако, низкий occupancy и высокая задержка при обращении к памяти может означать что программа
написана не достаточно эффективно, и ей необходимы улучшения, при условии что это возможно сделать.

Чем больше на одном вычислителе warp "--- тем реже все warp оказываются в
состоянии <<ждем запрос памяти>> и тем реже вычислитель будет простаивать,
т.к. тем чаще у него находится рабочая группа в которой можно что-то
посчитать\cite{videocard-architecture}.

Если потоки из одного warp делают запрос к памяти, то эти
запросы склеются в столько запросов, сколькими кэш-линиями покрываются
запрошенные данные.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{screenshots/memory-res}
	\caption{Доступные ресурсы "--- память.}\label{mem}
\end{figure}

Другими словами, если потоки запрашивают данные, которые в памяти лежат подряд, то 
достигнутая пропускная способность будет максимальная так как запросы <<склеются>>.
Размер кэш-линии обычно от 32 до 128 байт.

Если приложение использует OpenCL 1.x, то размеры NDRange должны нацело (без остатка) делиться на размеры рабочих групп.
Там, где данные образуют NDRange с другим размером, необходимо самостоятельно изменить их чтобы выполнялось это условие,
например, добавлением нулей или средних значений, которые не будут значимо влиять на результат вычислений\cite{work-groups}.

В OpenCL 2.0 появилась новая возможность, в которой устранена данная проблема. Речь идет о так называемых неоднородных рабочих группах: 
выполняемый модуль OpenCL 2.0 может разделить NDRange на рабочие группы неоднородного размера по любому измерению. 
Если разработчик укажет размер рабочей группы, на который размер NDRange не делится нацело, выполняемый модуль разделит NDRange таким образом, 
чтобы создать как можно больше рабочих групп с указанным размером, а остальные рабочие группы будут иметь другой размер. 
Например, для NDRange размером 1918x1078 рабочих элементов при размере рабочей группы 16x16 элементов среда выполнения OpenCL 2.0 разделит NDRange, 
как показано на приведенном ниже рисунке~\ref{ndrange}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{screenshots/ndrange-selection}
	\caption{Разделение NDRange на рабочие группы разных размеров.}\label{ndrange}
\end{figure}


\subsection{Настройка среды разработки}
В данном разделе будет рассмотрен процесс настройки среды разработки и создания первого OpenCL-проекта.

В качестве среды разработки для программирования с использованием OpenCL выбрана Microsoft Visual Studio, 
язык программирования "--- C++.

На компьютер была установлена реализация OpenCL от Nvidia: Nvidia GPU Computing SDK. А
также программа CMake, являющаяся независимым от платформы инструментом для сборки проектов~\ref{cmake}.

С помощью графического интерфейса выберем расположение файлов исходного кода и места сборки проекта.
Директория с исходными файлами должны содержать текстовые файлы CMakeLists из приложения~\ref{pril-2}.
Если OpenCL установлен корректно, то нажатие кнопки <<Configure>> выведет найденные на компьютеры
файлы, связанные с OpenCL. Нажмем <<Generate>>, и перейдем в папку с проектом, в котором можно увидеть созданный
файл .sln проекта Microsoft Visual Studio, сконфигурированного под OpenCL.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/cmake}
	\caption{Окно программы CMake-gui.}\label{cmake}
\end{figure}


\subsection{Инициализация OpenCL программы}
В данном разделе будут рассмотрены базовые функции, необходимые для инициализации параллельной программы
с использованием OpenCL. Данные функции будут предварять задачи из следующих разделов. 

Рассмотрим пример, взятый из руководства по OpenCL~\cite{opencl-guide}. С полным кодом,
содержащим комментарии, переведенными на русский язык, можно ознакомиться в приложении~\ref{pril-1},
файл HelloWorld.cpp. Обратим внимание на последовательность действий в функции main().
Многие понятия из данного раздела подробно описаны в~\ref{general-opencl}.

Сначала с помощью функции \verb|CreateContext()| создается контекст на основе
первой найденной на компьютере платформы. Далее для первого доступного устройства в контексте
создается командная очередь \newline \verb|clCreateCommandQueue()|, 
а в случае неудачи запускается функция очистки и программа завершается с кодом ошибки 1.

Из файла с исходным кодом kernel HelloWorld.cl создается OpenCL программа \verb|clCreateProgramWithSource()|. 
После этого создается и сам kernel на основе созданной <<программы>> \verb|clCreateKernel()|.

После этого создаются объекты памяти для конкретной задачи \newline \verb|clCreateBuffer()|, и каждый из них поочередно 
загружается в kernel с помощью \verb|clSetKernelArg()|.
Затем kernel ставится в очередь на выполнение \verb|clEnqueueNDRangeKernel()|, и после завершения работы, выводим в консоль буфер"=результат,
являющимся результатом выполнения данной OpenCL программы \verb|clEnqueueReadBuffer()|.

\subsection{Задачи на одномерных массивах}
Решим задачи на одномерных массивах, демонстрируя разные возможности OpenCL.

\subsubsection{Вычисление суммы ряда}
Одна из самых тривиальных задач "--- посчитать сумму двух векторов.
Даны векторы \textbf{a} и \textbf{b}, на основе их суммы должен получиться вектор \textbf{result}.
%https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/clEnqueueNDRangeKernel.html

\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/HelloWorld.cl}

Рассмотрим kernel для решения данной задачи. Каждый work-item, исполняя копию kernel, узнает
свой глобальный ID в рабочем пространстве с помощью \verb|get_global_id(0)|. Таким образом
он читает данные из видеопамяти соответствуя своему индексу. В основной программе выбран NDRange
размерности 1024 и \verb|local_work_size|, размер рабочей группы, равный 32.
Эти параметры переданы в функцию \verb|clEnqueueNDRangeKernel()|\cite{enqueue}.

Пусть значения компонент первого вектор соответствуют номеру компоненты начиная с 0, а значения
компонент второго вектора соответствуют удвоенному номеру компоненты в векторе. Результат работы
данной программы представлен на изображении~\ref{sum-vector}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{screenshots/sum-vector}
	\caption{Результат работы программы по суммированию двух векторов.}\label{sum-vector}
\end{figure}

Проблем с обращением к памяти не имеется так как потоки в рамках одной рабочей группы обращаются
к последовательным данным, кэш"=линиям.

\begin{table}[h]
	\centering
	\caption{Сравнение времени работы kernel суммирования векторов размерности 2097152}
	\begin{tabular}{|c|c|c|}
			\hline Устройство  & Размер рабочей группы & Время работы, мкс \\
			\hline NVIDIA Geforce 1050 Ti & 1 & 25026 \\
			\hline NVIDIA Geforce 560 Ti & 1 & 44150 \\
			\hline NVIDIA Geforce 1050 Ti & 128 & 14345 \\
			\hline NVIDIA Geforce 560 Ti & 128 & 29097 \\
			\hline
	\end{tabular}
\end{table}


\subsubsection{Нахождение максимального префикса}
Усложним задачу, найдя для всего массива максимальную сумму на префиксе.
То есть, значение префиксной суммы для каждого элемента в массиве должно определяться суммой
всех предыдущих элементов массива включая его самого.

Данная задача решается одним проходом по массиву в последовательных решениях, но требует значительной модификации
алгоритма для параллельных вычислений на видеокарте.

Решим задачу рекурсивно. 
Пусть входными данными для kernel является: массив \verb|a| с N заданными числами, массив \verb|prefs|, содержащий N нулей.
На каждом шаге найдем сумму и максимальный префикс для всех элементов 
на подмножестве, соответствующем некоторой рабочей группе. Все потоки в warp запрашивают данные с глобальной
памяти, и после этого первый поток посчитает сумму элементов и максимальный префикс по всей рабочей группе, и
записав результат в массивы \verb|sums| и \verb|prefs| размерностью меньше в \verb|WORK_GROUP_SIZE| раз. 
В результате выполнения kernel получим сумму всех элементов и максимальный префикс в данной рабочей группе. Используя полученные
данные, запустим данный kernel, в котором массив \verb|a| это массив \verb|sums| на предыдущем шаге.
Повторим процедуру, пока в массиве \verb|prefs| не останется 1 элемент "--- максимальный префикс на всем массиве.

Код kernel для решения данной задачи:
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/prefixsum.cl}

\begin{table}[h]
	\centering
	\caption{Сравнение времени работы kernel нахождения максимального префикса на массиве с 2097152 элементами}
	\begin{tabular}{|c|c|c|}
			\hline Устройство  & Размер рабочей группы & Время работы, мкс \\
			\hline NVIDIA Geforce 1050 Ti & 128 & 17345 \\
			\hline NVIDIA Geforce 560 Ti & 128 & 32097 \\
			\hline
	\end{tabular}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/prefix-exec}
	\caption{Результат работы программы по нахождению максимального префикса на массиве из 32 элементов. 
	Элементы, состоящие в рамке, образуют его, в то время как сумма всех элементов в массиве
	будет меньше этого значения}\label{prefix-exec}
\end{figure}

В данном фрагменте использовались функции идентификации рабочей группы, позволяющие узнать номер данной
рабочей группы и ее размер, а также номер элемента, соответствующего потоку в данной рабочей группе.  

\subsection{Задачи на двумерных массивах}
Следующий набор задач использует в качестве входных данных двумерный массив. 

\subsubsection{Транспонирование матрицы}
Решим задачу транспонирования матрицы. Задача сводится к считыванию и записи данных в память, но как было описано ранее в разделе~\ref{effectiveness}, эти
операции являются очень медленными. 

В простейшей реализация kernel будет выглядеть следующим образом:

\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Transpose.cl}

Результат работы программы представлен на изображении~\ref{transpose-res}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/transpose-res-new}
	\caption{Результат работы программы по транспонированию матрицы. Для выравнивания элементам меньше 10 добавлены незначащие нули}\label{transpose-res}
\end{figure}

В этом kernel данные считываются построчно, и записываются в столбец. Заметим, что
операция считывания, очевидно, происходит в одной кэш-линии, и все work-item в рабочей группе за 
1 глобальную операцию получат данные из исходной матрицы. Запись, напротив, происходит в разные строки,
и данные не могут находиться в одной кэш-линии. Следовательно, на каждый активный warp
произойдет $~$32 глобальные операции записи, и это сильно замедлит выполнение алгоритма.

Данную проблему можно решить использованием локальной памяти для транспонирования в соответствии
с изображением~\ref{tile}. Задача переносится на tiles (плитки), которые создаются в локальной памяти, 
и поэтому доступ к ним происходит быстро\cite{matmul}. 

Задача условно делится на 3 части, разделенные функцией <<барьер>>:
\begin{enumerate}
	\item считывание из глобальной памяти в локальную (tile);
	\item транспонирование в локальной памяти (tile);
	\item запись из локальной памяти (tile) в глобальную.
\end{enumerate}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/tile-transpose}
	\caption{Транспонирование матрицы с использованием <<плиток>>.}\label{tile}
\end{figure}

Код оптимизированного kernel:

\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Transpose-tile.cl}

В данном коде используется функция \verb|get_local_id()| для определения позиции внутри рабочей группы,
и, соответственно, получения элемента плитки, с которым будет работать текущий work-item

\begin{table}[h]
	\centering
	\caption{Сравнение времени работы оптимизированного kernel транспонирования матрицы на разных 
	устройствах для входных данных размерности 4096x2048, рабочие группы 32x32}
	\begin{tabular}{|c|c|}
			\hline Устройство  & Время работы, мкс \\
			\hline NVIDIA Geforce 1050 Ti & 21648 \\
			\hline NVIDIA Geforce 560 Ti & 55519 \\
			\hline
	\end{tabular}
\end{table}


\subsubsection{Умножение матриц}
Задача умножения матриц является одной из типовых задач, решение которых имеет огромное преимущество 
при использовании видеокарты для ее вычисления~\ref{mul-naive}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/matmul-naive}
	\caption{Умножение матриц A ($M \times K$) и B ($K \times N$).}\label{mul-naive}
\end{figure}

Как и в предыдущей задаче про транспонирование, рассмотрим наиболее простое решение задачи умножения матрицы A на матрицу B.
Каждый поток для соответствующей ему ячейки результирующей матрицы будет обрабатывать строку и столбец исходных матриц.

\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Matmul-naive.cl}

Можно заметить, что для каждой ячейки результирующей матрицы ($N \times M$ ячеек) происходит $2 \times K$ 
обращений к памяти, в массиве A он эффективный так как его элементы берутся последовательно, а в массиве B "---
нет, и при получении элементов из столбца может быть выполнено до 32 обращений к памяти вместо одного. Таким образом происходит
O($M \times N \times K$) обращений к памяти, что является существенным фактором медленной работы алгоритма\cite{matmul}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/matmul-exec}
	\caption{Умножение матриц A ($15 \times 15$) и B ($15 \times 15$). Снизу изображен результат выполнения}\label{mul-exec}
\end{figure}

\begin{table}[h]
	\centering
	\caption{Сравнение времени работы <<наивного>> kernel умножения матриц размерности $1024\times1024$}
	\begin{tabular}{|c|c|c|}
			\hline Устройство  & Размер рабочей группы & Время работы, мкс \\
			\hline NVIDIA Geforce 1050 Ti & $1\times1$ & 577230 \\
			\hline NVIDIA Geforce 560 Ti & $1\times1$ & 1274058 \\
			\hline NVIDIA Geforce 1050 Ti & $32\times16$ & 27290 \\
			\hline NVIDIA Geforce 560 Ti & $32\times16$ & 49821 \\
			\hline
	\end{tabular}
\end{table}

От выбора размера рабочей группы сильно зависит время выполнения алгоритма. Данный результат можно объяснить 
тем что в первом случае каждое вычислительное устройство считывает из памяти данные для вычисления одной ячейки, а остальные потоки
не работают. В случае рабочей группы $32\times16$, напротив, в каждом вычислительном устройстве активны все 32 ядра, и во время выполнения
создается 16 warp для каждой рабочей группы, тем самым возможно переключение контекста для сокрытия задержки к памяти.   

Оптимизируем алгоритм, использовав модифицированную версию подхода, представленного в предыдущей задаче. Разобьем задачу на
фрагменты в локальной памяти, используемые потоками одной рабочей группы~\ref{mul-smart}. Таким образом,
одна рабочая группа использует локальную память для вычисления соответствующих ей плиток в наборе строк и столбцов

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{screenshots/matmul-smart}
	\caption{Использование локальной памяти при умножении матриц.}\label{mul-smart}
\end{figure}

Преимущество подобного решения "--- снижение длительности операций считывания памяти за счет того
что каждый warp работает в собственной локальной памяти, а также за счет считывания данных построчно (см. раздел~\ref{effectiveness}).

\begin{table}[h]
	\centering
	\caption{Сравнение времени работы оптимизированного kernel умножения матриц размерности $1024\times1024$}
	\begin{tabular}{|c|c|c|}
			\hline Устройство  & Размер рабочей группы & Время работы, мкс \\
			\hline NVIDIA Geforce 1050 Ti & $K\times32$ & 13845 \\
			\hline NVIDIA Geforce 560 Ti & $K\times32$ & 30107 \\
			\hline
	\end{tabular}
\end{table}

Код kernel для эффективного умножения матриц:
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Matmul-smart.cl}

\newpage
%===========================================
% Раздел "Заключение"
\conclusion
В настоящей работе была изучена технология OpenCL для параллельных вычислений на видеокарте.
Корректно построенные параллельные алгоритмы на видеокарте показывают высокую производительность за
счет использования большого числа ядер. Эффективность алгоритма зависит от поддержания массового
параллелизма, корректного ветвления кода, а также правильного доступа к памяти.

Была изучена архитектура видеокарты и связанные с ней понятия OpenCL, а также решены некоторые задачи
с исходными данных разных размерностей
путем написания высокопроизводительных OpenCL программ, состоящих из хостовых и kernel частей.
Проведено сравнение времени выполнения алгоритма на различных конфигурациях OpenCL программы и
моделях видеокарт.

%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением

\appendix

\section{Листинг программы}\label{pril-1}
Код хостовой части программы для задач с входными данными в виде одномерного массива.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/HelloWorld.cpp}
Kernel для задачи вычисления суммы двух векторов.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/HelloWorld.cl}

Kernel для задачи вычисления максимального префикса в массиве.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/prefixsum.cl}

Код хостовой части программы для задач с входными данными в виде двумерного массива.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Transpose.cpp}
Kernel для <<наивной>> реализации задачи транспонирования матрицы.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/transpose.cl}
Kernel для эффективной реализации задачи транспонирования матрицы.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/transpose-tile.cl}

Kernel для <<наивной>> реализации задачи умножения матриц.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Matmul-naive.cl}
Kernel для эффективной реализации задачи умножения матриц.
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/Matmul-smart.cl}

\section{Листинг сборочных файлов CMake}\label{pril-2}
Код сборочного файла CMakeLists.txt
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/CMakeLists.txt}

Пример сборочного файла CMakeLists.txt для проекта с транспонированием матрицы
\VerbatimInput[fontsize=\small, numbers=left, numbersep=2pt]{code/cmake/transpose/CMakeLists.txt}

\end{document}
